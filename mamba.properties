batch_id = 'testing' ### The run_id you want to use
data1_name = data1_test  ###First dataset name
data2_name = data2_test  ###Second dataset name.
sql_flavor = sqlite ###what flavor of sql are you using? current options: sqlite or postgres.  if using postgres, add in db_user, db_port, db_password, and db_host as entries
db_name = mamba_db ###database name
outputPath = C:\\users\\cuffe002\\desktop\\projects\\mamba2\\output ##the path for our output files
debugmode = False ###run in debug mode?
block_file_name = block_names.csv ###The file (including .csv ending) where the names of the blocking variables live
create_db_chunksize = 100000  ###the size of the chunks we want to read in
inputPath = C:\\users\\cuffe002\\desktop\\projects\\mamba2\\data ##path to your data
training_data_name = training_data  ###name of the training data.csv file.  don't include .csv ending
rf_jobs = 1  ###Number of jobs in the random forest
clerical_review_candidates = True  ##do we want to generate clerical review candidates
clerical_review_threshold = .3  ###What's the lowest predicted probability you want returned for a clerical review candidate?
match_threshold = .5  ##what is the match threshold you want to look at
chatty_logger = True  ##if on, logger logs after every block
log_file_name = mamba_test_log ##the anme of your log file
numWorkers = 3 ##number of workers in the Runner object
prediction = True ##are we generating match predictions?
scoringcriteria = accuracy
ignore_duplicate_ids = False ###are we trying to depulicate the same dataset? This assumes the IDs are the same on each set (so a record in A has the same record in B)
use_logit = True ####Use the logit.  If you do this, scoring will be converted to accuracy (only built-in feature)
date_format = %Y-%m-%d   ###format for any dates
stem_phrase = 'False' ###Use the stemming/phrasing feature?
parse_address = True   ###are we using the address parsing?
address_column_data1_test = address_unparsed   ###name of address column needed parsed in the first dataset
address_column_data2_test = address_raw ###name of address column needed parsed in the second dataset
use_remaining_parsed_address = True ##are we using thee leftover address parsing info to match?
 ####if for a dataset you don't want to standardize for a particular dataset, don't include a json entry
address_to_standardize = {"data1_test":"street_address,address_unparsed" , "data2_test":"address,address_raw"} 
###Are we running a custom model?
use_custom_model = True
###What imputation model are we using: Imputation = impute values using an iterative imputer,Nnominal means cut fuzzy vars
imputation_method = Nominal